{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:23: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\Sudee\\AppData\\Local\\Temp\\ipykernel_8748\\3198217925.py:23: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  sys.path.append(os.path.abspath(\"D:\\Projects\\Amazon-ML-Challenge-Team-AutoGen-\\student_resource 3\\src\"))\n",
      "d:\\python3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from transformers import LayoutLMTokenizer, LayoutLMForTokenClassification\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from albumentations import Compose, Resize, RandomBrightnessContrast, Normalize\n",
    "import torch  # For CUDA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import mlflow  # For experiment tracking\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Assuming your script is in the same directory as src\n",
    "sys.path.append(os.path.abspath(\"D:\\Projects\\Amazon-ML-Challenge-Team-AutoGen-\\student_resource 3\\src\"))\n",
    "\n",
    "from utils import download_images  # Assuming this function is defined in utils.py\n",
    "from constants import entity_unit_map, allowed_units  # Importing the unit map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'dataset/train.csv'\n",
    "TEST_CSV = 'dataset/test.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_links, save_dir, batch_size):\n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    existing_images = [img for img in os.listdir(save_dir) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    existing_count = len(existing_images)\n",
    "\n",
    "    # Determine the batch ID to start from\n",
    "    next_batch_id = (existing_count // batch_size) + 1\n",
    "    print(f\"Starting from batch {next_batch_id}...\")\n",
    "\n",
    "    downloaded_images = []\n",
    "    for idx, link in enumerate(image_links):\n",
    "        # Image path format: save_dir/image_<batch_id>_<image_id>.jpg\n",
    "        image_id = existing_count + idx\n",
    "        batch_id = (image_id // batch_size) + 1\n",
    "        image_filename = f\"image_{batch_id}_{image_id}.jpg\"\n",
    "        image_path = os.path.join(save_dir, image_filename)\n",
    "\n",
    "        # Download and save the image\n",
    "        try:\n",
    "            response = requests.get(link, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(image_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                downloaded_images.append(image_path)\n",
    "            else:\n",
    "                print(f\"Failed to download image from {link}, status code {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {link}: {e}\")\n",
    "\n",
    "    return downloaded_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "def preprocess_images(image_paths, target_size=(224, 224)):\n",
    "    augment = A.Compose([\n",
    "        A.Resize(height=target_size[0], width=target_size[1]),\n",
    "        A.RandomBrightnessContrast(p=0.2)\n",
    "    ])\n",
    "    \n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        # Open the image, convert to RGB, and convert to numpy array\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Apply the augmentations\n",
    "        augmented = augment(image=image)\n",
    "        images.append(augmented['image'])\n",
    "    \n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text extraction using pytesseract\n",
    "def extract_text_from_images(image_paths):\n",
    "    texts = []\n",
    "    for path in image_paths:\n",
    "        image = Image.open(path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_abbreviations = {\n",
    "    # For 'item_weight' and 'maximum_weight_recommendation'\n",
    "    'gram': ['g', 'gr', 'gm', 'grams', 'grm'],\n",
    "    'kilogram': ['kg', 'kilograms', 'kgs'],\n",
    "    'milligram': ['mg', 'milligrams', 'mgs'],\n",
    "    'microgram': ['µg', 'mcg', 'micrograms'],\n",
    "    'ounce': ['oz', 'ounces', 'ozs'],\n",
    "    'pound': ['lb', 'lbs', 'pounds'],\n",
    "    'ton': ['t', 'tons', 'tonne', 'tonnes'],\n",
    "\n",
    "    # For 'item_volume'\n",
    "    'millilitre': ['ml', 'milliliters', 'millilitres'],\n",
    "    'litre': ['l', 'lit', 'liters', 'litres'],\n",
    "    'cubic_centimetre': ['cc', 'cm³', 'cubic centimeters', 'cubic centimetres'],\n",
    "    'cubic_metre': ['m³', 'cubic meters', 'cubic metres'],\n",
    "    'gallon': ['gal', 'gallons'],\n",
    "    'quart': ['qt', 'quarts'],\n",
    "    'pint': ['pt', 'pints'],\n",
    "    'cup': ['c', 'cups'],\n",
    "\n",
    "    # For 'voltage'\n",
    "    'volt': ['v', 'volts'],\n",
    "    'kilovolt': ['kv', 'kilovolts'],\n",
    "    'millivolt': ['mv', 'millivolts'],\n",
    "\n",
    "    # For 'wattage'\n",
    "    'watt': ['w', 'watts'],\n",
    "    'kilowatt': ['kw', 'kilowatts'],\n",
    "    'megawatt': ['mw', 'megawatts'],\n",
    "    'gigawatt': ['gw', 'gigawatts'],\n",
    "\n",
    "    # For 'height', 'depth', and 'width'\n",
    "    'millimetre': ['mm', 'millimeters', 'millimetres'],\n",
    "    'centimetre': ['cm', 'centimeters', 'centimetres'],\n",
    "    'metre': ['m', 'meters', 'metres'],\n",
    "    'kilometre': ['km', 'kilometers', 'kilometres'],\n",
    "    'inch': ['in', 'inches'],\n",
    "    'foot': ['ft', 'feet'],\n",
    "    'yard': ['yd', 'yards'],\n",
    "    'mile': ['mi', 'miles'],\n",
    "\n",
    "    # Other common units\n",
    "    'degree_celsius': ['°C', 'C', 'degrees Celsius'],\n",
    "    'degree_fahrenheit': ['°F', 'F', 'degrees Fahrenheit'],\n",
    "    'calorie': ['cal', 'calories'],\n",
    "    'kilocalorie': ['kcal', 'kcals'],\n",
    "    'joule': ['j', 'joules'],\n",
    "    'pascal': ['Pa', 'pascals'],\n",
    "    'bar': ['bar', 'bars'],\n",
    "    'psi': ['psi', 'pounds per square inch'],\n",
    "    'newton': ['N', 'newtons'],\n",
    "    'fluid_ounce': ['fl oz', 'fluid ounces'],\n",
    "}\n",
    "\n",
    "def standardize_unit(value, entity_name):\n",
    "    for unit, abbreviations in unit_abbreviations.items():\n",
    "        for abbr in abbreviations:\n",
    "            if re.search(r'\\b' + re.escape(abbr) + r'\\b', value):\n",
    "                standardized_value = re.sub(abbr, unit, value)\n",
    "                return standardized_value\n",
    "    return value  # If no abbreviation found, return the original value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entity_value(value):\n",
    "    # Remove all non-digit and non-dot characters\n",
    "    cleaned_value = re.sub(r'[^\\d.]', '', value)\n",
    "    \n",
    "    # Split by multiple dots and take the first valid number if necessary\n",
    "    parts = cleaned_value.split('.')\n",
    "    \n",
    "    if len(parts) > 2:\n",
    "        # If there are multiple dots, recombine the first two parts as the integer and decimal\n",
    "        cleaned_value = parts[0] + '.' + ''.join(parts[1:])\n",
    "    else:\n",
    "        # Otherwise, keep as is\n",
    "        cleaned_value = '.'.join(parts)\n",
    "    \n",
    "    try:\n",
    "        return float(cleaned_value)\n",
    "    except ValueError:\n",
    "        # Return NaN if the conversion fails\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_resnet_model():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(1, activation='linear')(x)  # For regression\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Mixed Precision\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_layoutlm_model(num_labels):\n",
    "    tokenizer = LayoutLMTokenizer.from_pretrained('microsoft/layoutlm-base-uncased')\n",
    "    model = LayoutLMForTokenClassification.from_pretrained('microsoft/layoutlm-base-uncased', num_labels=num_labels)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_resnet(model, images):\n",
    "    predictions = model.predict(images)\n",
    "    return predictions.flatten()  # For regression, return raw predictions\n",
    "\n",
    "def predict_with_layoutlm(model, tokenizer, texts):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
    "    outputs = model(**encodings)\n",
    "    return np.argmax(outputs.logits.detach().numpy(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(test_df, resnet_model, layoutlm_model, tokenizer):\n",
    "    image_links = test_df['image_link'].tolist()\n",
    "    image_paths = download_images(image_links)\n",
    "    images = preprocess_images(image_paths)\n",
    "    \n",
    "    # Feature Extraction with ResNet\n",
    "    resnet_predictions = predict_with_resnet(resnet_model, images)\n",
    "    \n",
    "    # Text Extraction and LayoutLM Predictions\n",
    "    texts = extract_text_from_images(image_paths)\n",
    "    cleaned_texts = [standardize_unit(text, None) for text in texts]  # Pass None for entity_name if not needed\n",
    "    layoutlm_predictions = predict_with_layoutlm(layoutlm_model, tokenizer, cleaned_texts)\n",
    "    \n",
    "    # Combine predictions\n",
    "    predictions = [f\"{resnet_pred} {layoutlm_pred}\" for resnet_pred, layoutlm_pred in zip(resnet_predictions, layoutlm_predictions)]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(test_df, predictions, submission_file='submission.csv'):\n",
    "    test_df['prediction'] = predictions\n",
    "    test_df.to_csv(submission_file, index=False)\n",
    "    print(f\"Predictions saved to {submission_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_training(train_df, model_save_path, batch_size=100, max_batches=30):\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    for batch in range(1, max_batches + 1):\n",
    "        print(f\"Processing batch {batch}...\")\n",
    "        \n",
    "        # Download and preprocess images for the current batch\n",
    "        image_links = train_df['image_link'].head(batch * batch_size).tolist()\n",
    "        save_dir = './images'  # Directory where images will be saved\n",
    "        image_paths = download_images(image_links, save_dir, batch_size)  # Pass batch_size to the function\n",
    "        images = preprocess_images(image_paths)\n",
    "        \n",
    "        # Clean and process labels\n",
    "        labels = train_df['entity_value'].head(batch * batch_size).apply(clean_entity_value).values\n",
    "        labels = labels[~np.isnan(labels)]  # Remove NaNs if any\n",
    "        \n",
    "        # Create TensorFlow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "        dataset = dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "        \n",
    "        # Split dataset into training and validation sets\n",
    "        validation_size = int(0.2 * len(images))\n",
    "        train_size = len(images) - validation_size\n",
    "        train_dataset = dataset.take(train_size)\n",
    "        validation_dataset = dataset.skip(train_size)\n",
    "        \n",
    "        # Initialize and compile the model\n",
    "        model = initialize_resnet_model()\n",
    "        \n",
    "        # Learning Rate Scheduling\n",
    "        def lr_scheduler(epoch, lr):\n",
    "            if epoch < 10:\n",
    "                return lr\n",
    "            else:\n",
    "                return lr * tf.math.exp(-0.1)\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "            ModelCheckpoint(model_save_path, save_best_only=True, monitor='val_accuracy'),\n",
    "            LearningRateScheduler(lr_scheduler)\n",
    "        ]\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=5,\n",
    "            validation_data=validation_dataset if len(validation_dataset) > 0 else None,  # Skip if empty\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        accuracy = max(history.history.get('val_accuracy', [0]))  # Handle empty val_accuracy\n",
    "        print(f\"Batch {batch} Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = load_model(model_save_path)\n",
    "    \n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1...\n",
      "Starting from batch 1...\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Adjust NUM_CLASSES as needed\n",
    "    NUM_CLASSES = 30  # Example number of classes, adjust based on your task\n",
    "    model_save_path = 'best_model.keras'\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    \n",
    "    # Cumulative Training\n",
    "    best_model = cumulative_training(train_df, model_save_path, batch_size=100)\n",
    "    \n",
    "    # Initialize LayoutLM and ResNet models\n",
    "    tokenizer, layoutlm_model = initialize_layoutlm_model(num_labels=NUM_CLASSES)\n",
    "    resnet_model = load_model(model_save_path)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = generate_predictions(test_df, resnet_model, layoutlm_model, tokenizer)\n",
    "    \n",
    "    # Save predictions\n",
    "    save_predictions(test_df, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
