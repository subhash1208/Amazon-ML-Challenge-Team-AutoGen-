{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhash1208/Amazon-ML-Challenge-Team-AutoGen-/blob/main/Team_AutoGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ew9gfaRbCql-"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'constants'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_images  \u001b[38;5;66;03m# Assuming this function is defined in utils.py\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entity_unit_map, allowed_units  \u001b[38;5;66;03m# Importing the unit map\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m  \u001b[38;5;66;03m# For image processing\u001b[39;00m\n",
            "File \u001b[1;32md:\\Projects\\Amazon-ML-Challenge-Team-AutoGen-\\student_resource 3\\src\\utils.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'constants'"
          ]
        }
      ],
      "source": [
        "# Cell 1: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from src.utils import download_images  # Assuming this function is defined in utils.py\n",
        "from src.constants import entity_unit_map, allowed_units  # Importing the unit map\n",
        "import cv2  # For image processing\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "import torch  # For BERT and PyTorch\n",
        "from transformers import BertTokenizer, BertForTokenClassification  # For BERT model\n",
        "from transformers import Trainer, TrainingArguments  # For training the model\n",
        "import random\n",
        "import re\n",
        "from transformers import EarlyStoppingCallback  # For callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Load and Clean the Dataset\n",
        "# Load the training and test datasets\n",
        "train_df = pd.read_csv('dataset/train.csv')\n",
        "test_df = pd.read_csv('dataset/test.csv')\n",
        "\n",
        "# Display total null values in the dataset\n",
        "print(\"Total null values in training dataset:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "# Replace null values in entity_value with a specified string\n",
        "train_df['entity_value'].fillna('unknown', inplace=True)  # Replace NaN in entity_value with 'unknown'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Download Images\n",
        "# Download images for both train and test datasets\n",
        "download_images(train_df['image_link'].tolist(), download_folder='dataset/train_images')\n",
        "download_images(test_df['image_link'].tolist(), download_folder='dataset/test_images')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Display Random Images with Entity Name and Value\n",
        "# Display 5 random images with their entity names and values\n",
        "def display_random_images(df, num_images=5):\n",
        "    random_samples = df.sample(num_images)\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    for i, (index, row) in enumerate(random_samples.iterrows()):\n",
        "        img_path = os.path.join('dataset/train_images', row['image_link'].split('/')[-1])\n",
        "        img = plt.imread(img_path)\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"{row['entity_name']}: {row['entity_value']}\")\n",
        "        plt.axis('off')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "display_random_images(train_df, num_images=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Group Data by Entity Name\n",
        "# Group the training data by entity_name\n",
        "grouped_train_df = train_df.groupby('entity_name')\n",
        "\n",
        "# Display the grouped data (optional)\n",
        "for name, group in grouped_train_df:\n",
        "    print(f\"Entity Name: {name}, Number of Samples: {len(group)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Prepare Data for Model Training\n",
        "# Create a mapping for allowed units and their abbreviations\n",
        "unit_abbreviations = {\n",
        "    # For 'item_weight' and 'maximum_weight_recommendation'\n",
        "    'gram': ['g', 'gr', 'gm', 'grams', 'grm'],\n",
        "    'kilogram': ['kg', 'kilograms', 'kgs'],\n",
        "    'milligram': ['mg', 'milligrams', 'mgs'],\n",
        "    'microgram': ['µg', 'mcg', 'micrograms'],\n",
        "    'ounce': ['oz', 'ounces', 'ozs'],\n",
        "    'pound': ['lb', 'lbs', 'pounds'],\n",
        "    'ton': ['t', 'tons', 'tonne', 'tonnes'],\n",
        "\n",
        "    # For 'item_volume'\n",
        "    'millilitre': ['ml', 'milliliters', 'millilitres'],\n",
        "    'litre': ['l', 'lit', 'liters', 'litres'],\n",
        "    'cubic_centimetre': ['cc', 'cm³', 'cubic centimeters', 'cubic centimetres'],\n",
        "    'cubic_metre': ['m³', 'cubic meters', 'cubic metres'],\n",
        "    'gallon': ['gal', 'gallons'],\n",
        "    'quart': ['qt', 'quarts'],\n",
        "    'pint': ['pt', 'pints'],\n",
        "    'cup': ['c', 'cups'],\n",
        "\n",
        "    # For 'voltage'\n",
        "    'volt': ['v', 'volts'],\n",
        "    'kilovolt': ['kv', 'kilovolts'],\n",
        "    'millivolt': ['mv', 'millivolts'],\n",
        "\n",
        "    # For 'wattage'\n",
        "    'watt': ['w', 'watts'],\n",
        "    'kilowatt': ['kw', 'kilowatts'],\n",
        "    'megawatt': ['mw', 'megawatts'],\n",
        "    'gigawatt': ['gw', 'gigawatts'],\n",
        "\n",
        "    # For 'height', 'depth', and 'width'\n",
        "    'millimetre': ['mm', 'millimeters', 'millimetres'],\n",
        "    'centimetre': ['cm', 'centimeters', 'centimetres'],\n",
        "    'metre': ['m', 'meters', 'metres'],\n",
        "    'kilometre': ['km', 'kilometers', 'kilometres'],\n",
        "    'inch': ['in', 'inches'],\n",
        "    'foot': ['ft', 'feet'],\n",
        "    'yard': ['yd', 'yards'],\n",
        "    'mile': ['mi', 'miles'],\n",
        "\n",
        "    # Other common units\n",
        "    'degree_celsius': ['°C', 'C', 'degrees Celsius'],\n",
        "    'degree_fahrenheit': ['°F', 'F', 'degrees Fahrenheit'],\n",
        "    'calorie': ['cal', 'calories'],\n",
        "    'kilocalorie': ['kcal', 'kcals'],\n",
        "    'joule': ['j', 'joules'],\n",
        "    'pascal': ['Pa', 'pascals'],\n",
        "    'bar': ['bar', 'bars'],\n",
        "    'psi': ['psi', 'pounds per square inch'],\n",
        "    'newton': ['N', 'newtons'],\n",
        "    'fluid_ounce': ['fl oz', 'fluid ounces'],\n",
        "}\n",
        "\n",
        "# Prepare the data for BERT encoding\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_data(df):\n",
        "    return tokenizer(df['entity_value'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "train_encodings = encode_data(train_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Define the Model\n",
        "# Load BERT model for token classification\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(entity_unit_map))\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        ")\n",
        "\n",
        "# Create a Trainer instance with EarlyStoppingCallback\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_encodings,  # You will need to create a proper dataset class\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop training if no improvement\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Train the Model\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Evaluate the Model\n",
        "# Evaluate the model on the training set\n",
        "train_predictions = trainer.predict(train_encodings)\n",
        "train_preds = np.argmax(train_predictions.predictions, axis=2)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(train_df['entity_value'], train_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Make Predictions on Test Set\n",
        "# Prepare test data for predictions\n",
        "test_encodings = encode_data(test_df)\n",
        "\n",
        "# Make predictions\n",
        "test_predictions = trainer.predict(test_encodings)\n",
        "test_preds = np.argmax(test_predictions.predictions, axis=2)\n",
        "\n",
        "# Map predictions back to entity names\n",
        "test_df['prediction'] = [entity_unit_map[entity][pred] for entity, pred in zip(test_df['entity_name'], test_preds)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Create Submission File\n",
        "# Create a DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    'index': test_df['index'],\n",
        "    'prediction': test_df['prediction']\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file 'submission.csv' created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPjfAx+DuzGzvd4qBpOxD2h",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
